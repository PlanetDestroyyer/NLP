{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnDX1sED6gG5sKPfC03M4L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PlanetDestroyyer/NLP/blob/main/Tokenization_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgBm6z2w5jba"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gULdtAYq6YE3",
        "outputId": "d1d004ff-e97d-48e1-bf83-31857a7eba5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"\n",
        "Hello My Name is Pranav.\n",
        "This is Tokenization Examples.\n",
        "I am From India , specifically from western part of India\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "AZ3bhxIU52mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0CMcItC6Tdy",
        "outputId": "f967cf2d-22ec-4dec-cfa0-700da20a1414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hello My Name is Pranv.\n",
            "This is Tokenization Examples.\n",
            "I am From India , specifically from western part of India\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentence tokenization\n"
      ],
      "metadata": {
        "id": "l1eBMyty8qO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize"
      ],
      "metadata": {
        "id": "iCJE9hLG6Dkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "DoR2qbS76HEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index,sentence in enumerate(sentences):\n",
        "  print(f\"{index} : {sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfczkEmW6Puu",
        "outputId": "a6760dc3-a854-4964-8be1-5ca7cae7164f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : \n",
            "Hello My Name is Pranv.\n",
            "1 : This is Tokenization Examples.\n",
            "2 : I am From India , specifically from western part of India\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word tokenization"
      ],
      "metadata": {
        "id": "K1NFlTTq87pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "JZ2jihGI6hI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "h8VprP3M9Aky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, word in enumerate(words):\n",
        "  print(f\"{index} : {word} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax6kVCrK9Elc",
        "outputId": "00f3baee-381f-465a-ea1c-3be05a744f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : Hello \n",
            "1 : My \n",
            "2 : Name \n",
            "3 : is \n",
            "4 : Pranv \n",
            "5 : . \n",
            "6 : This \n",
            "7 : is \n",
            "8 : Tokenization \n",
            "9 : Examples \n",
            "10 : . \n",
            "11 : I \n",
            "12 : am \n",
            "13 : From \n",
            "14 : India \n",
            "15 : , \n",
            "16 : specifically \n",
            "17 : from \n",
            "18 : western \n",
            "19 : part \n",
            "20 : of \n",
            "21 : India \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "UkWQjPua9Qe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_puc = wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "id": "UV1IhZ4e9eP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index,word in enumerate(words_puc):\n",
        "  print(f\"{index} : {word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxcgvDkI9i4u",
        "outputId": "3d1d16bd-95ed-418e-def0-a5a487675afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : Hello\n",
            "1 : My\n",
            "2 : Name\n",
            "3 : is\n",
            "4 : Pranv\n",
            "5 : .\n",
            "6 : This\n",
            "7 : is\n",
            "8 : Tokenization\n",
            "9 : Examples\n",
            "10 : .\n",
            "11 : I\n",
            "12 : am\n",
            "13 : From\n",
            "14 : India\n",
            "15 : ,\n",
            "16 : specifically\n",
            "17 : from\n",
            "18 : western\n",
            "19 : part\n",
            "20 : of\n",
            "21 : India\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "LnyBqNq79oAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokeiner = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "w6vfU1Jv93tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokeiner.tokenize(corpus)"
      ],
      "metadata": {
        "id": "zXOzGr0g97_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index,word in enumerate(tokens):\n",
        "  print(f\"{index} : {word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_DCXPap9-dh",
        "outputId": "c11063f2-a8ef-4379-8af5-50b821255312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : Hello\n",
            "1 : My\n",
            "2 : Name\n",
            "3 : is\n",
            "4 : Pranv.\n",
            "5 : This\n",
            "6 : is\n",
            "7 : Tokenization\n",
            "8 : Examples.\n",
            "9 : I\n",
            "10 : am\n",
            "11 : From\n",
            "12 : India\n",
            "13 : ,\n",
            "14 : specifically\n",
            "15 : from\n",
            "16 : western\n",
            "17 : part\n",
            "18 : of\n",
            "19 : India\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "USrS3rR2-DLe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}